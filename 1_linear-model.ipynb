{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are often the first step in machine learning for biological sequence-to-function prediction. Despite their simplicity, they can provide strong baseline performance, interpretability, and insight into key features driving biological outcomes. In this notebook, we demonstrate how to apply linear regression and regularized models (like Lasso and Ridge) to predict sequence-derived properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using for the sequence-to-fitness prediction is from ProteinGym (https://proteingym.org), a collection of benchmarks aiming at comparing the ability of models to predict the effects of protein mutations. We first import essential libraries, and data from the fitness folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scikit-learn, a nice package for building and evaluating machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPSD = pd.read_csv(\"data_fitness/CAPSD_AAV2S_Sinai_2021.csv\")\n",
    "PHOT = pd.read_csv(\"data_fitness/PHOT_CHLRE_Chen_2023.csv\")\n",
    "POLG = pd.read_csv(\"data_fitness/POLG_DEN26_Suphatrakul_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first process the data from CAPSD dataset, doing one-hot encoding for the amino acid sequences and then the train-test split. We take the first 5000 entries to keep runtime manageable.\n",
    "\n",
    "mutated_sequence contains amino acid sequences.\n",
    "\n",
    "DMS_score is the experimentally measured fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = CAPSD[\"mutated_sequence\"].values[:5000]\n",
    "scores = CAPSD[\"DMS_score\"].values[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\") \n",
    "encoder = OneHotEncoder(categories=[amino_acids] * len(sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 14700)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_list = [list(seq) for seq in sequences]\n",
    "X = encoder.fit_transform(seq_list)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an 80/20 trainâ€“test split. Since fitness values vary in scale, we apply StandardScaler to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 14700), (1000, 14700))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train_noscale, y_test_noscale = train_test_split(\n",
    "    X, scores, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "y_train= y_scaler.fit_transform(y_train_noscale.reshape(-1, 1)).ravel()\n",
    "y_test = y_scaler.transform(y_test_noscale.reshape(-1, 1)).ravel()\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the helper function to print different metrics for evaluating machine learning models on the held-out test set:\n",
    "\n",
    "-R2 score (coefficient of determination)\n",
    "\n",
    "-Mean Squared Error (MSE)\n",
    "\n",
    "-Mean Absolute Error (MAE)\n",
    "\n",
    "Higher R2 score, lower MSE and MAE indicate a better machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"R2:\", r2_score(y_true, y_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_true, y_pred))\n",
    "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train four linear regression models and evaluate each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Regression\n",
    "A baseline model with no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Linear Regression ---\n",
      "R2: 0.5500420178990411\n",
      "MSE: 0.4567800668557709\n",
      "MAE: 0.5093609866269868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "evaluate_model(\"Linear Regression\", y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ridge Regression (L2 Regularization)\n",
    "Improves stability by shrinking coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ridge Regression (L2) ---\n",
      "R2: 0.5662414139903585\n",
      "MSE: 0.4403350619353897\n",
      "MAE: 0.5054188511765942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "evaluate_model(\"Ridge Regression (L2)\", y_test, y_pred_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Lasso Regression (L1 Regularization)\n",
    "Performs feature selection by driving some coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lasso Regression (L1) ---\n",
      "R2: 0.5996939056106281\n",
      "MSE: 0.40637537688335224\n",
      "MAE: 0.4959062595602137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/BioML-tut-llm/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:656: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.649005982596691, tolerance: 0.4000000000000002\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.0001, max_iter=500)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "evaluate_model(\"Lasso Regression (L1)\", y_test, y_pred_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Elastic Net (L1 + L2)\n",
    "Combines L1 regularization with L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Elastic Net (L1 + L2) ---\n",
      "R2: 0.5948361151840126\n",
      "MSE: 0.4113068192048775\n",
      "MAE: 0.49773791995906425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/BioML-tut-llm/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:656: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8980845054136353, tolerance: 0.4000000000000002\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "elastic = ElasticNet(alpha=0.0001, l1_ratio=0.5, max_iter=500)\n",
    "elastic.fit(X_train, y_train)\n",
    "y_pred_elastic = elastic.predict(X_test)\n",
    "evaluate_model(\"Elastic Net (L1 + L2)\", y_test, y_pred_elastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that plain Linear Regression performs the worst, while both L1 and L2 regularization substantially improve performance. This suggests that the unregularized model likely overfits due to the high dimensionality of the one-hot encoded sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioML-tut-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
