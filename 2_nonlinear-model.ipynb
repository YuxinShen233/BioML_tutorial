{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological systems are rarely linear, so nonlinear models can capture complex sequence-to-function relationships more effectively. In this notebook, we explore random forests (RF), support vector regression (SVR), and multi-layer perceptrons (MLP) for predicting functional properties from sequences. These models balance predictive power with interpretability, enabling us to model intricate biological patterns beyond what linear models can capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using for the sequence-to-fitness prediction is from ProteinGym (https://proteingym.org), a collection of benchmarks aiming at comparing the ability of models to predict the effects of protein mutations. We first import essential libraries, and data from the fitness folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scikit-learn, a nice package for building and evaluating machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:42:36.442357Z",
     "start_time": "2025-11-27T20:42:35.798101Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:42:37.009115Z",
     "start_time": "2025-11-27T20:42:36.443457Z"
    }
   },
   "outputs": [],
   "source": [
    "CAPSD = pd.read_csv(\"data_fitness/CAPSD_AAV2S_Sinai_2021.csv\")\n",
    "PHOT = pd.read_csv(\"data_fitness/PHOT_CHLRE_Chen_2023.csv\")\n",
    "POLG = pd.read_csv(\"data_fitness/POLG_DEN26_Suphatrakul_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first process the data from CAPSD dataset, doing one-hot encoding for the amino acid sequences and then the train-test split. We take the first 5000 entries to keep runtime manageable.\n",
    "\n",
    "mutated_sequence contains amino acid sequences.\n",
    "\n",
    "DMS_score is the experimentally measured fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:42:37.011853Z",
     "start_time": "2025-11-27T20:42:37.010004Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = CAPSD[\"mutated_sequence\"].values[:5000]\n",
    "scores = CAPSD[\"DMS_score\"].values[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:42:38.095284Z",
     "start_time": "2025-11-27T20:42:37.012836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 14700)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\") \n",
    "encoder = OneHotEncoder(categories=[amino_acids] * len(sequences[0]))\n",
    "seq_list = [list(seq) for seq in sequences]\n",
    "X = encoder.fit_transform(seq_list)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an 80/20 train–test split. Since fitness values vary in scale, we apply StandardScaler to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:42:38.105243Z",
     "start_time": "2025-11-27T20:42:38.096159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 14700), (1000, 14700))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train_noscale, y_test_noscale = train_test_split(\n",
    "    X, scores, test_size=0.2, random_state=42)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train= y_scaler.fit_transform(y_train_noscale.reshape(-1, 1)).ravel()\n",
    "y_test = y_scaler.transform(y_test_noscale.reshape(-1, 1)).ravel()\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the helper function to print different metrics for evaluating machine learning models on the held-out test set:\n",
    "\n",
    "-R2 score (coefficient of determination)\n",
    "\n",
    "-Mean Squared Error (MSE)\n",
    "\n",
    "-Mean Absolute Error (MAE)\n",
    "\n",
    "Higher R2 score, lower MSE and MAE indicate a better machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:42:38.154822Z",
     "start_time": "2025-11-27T20:42:38.106051Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"R2:\", r2_score(y_true, y_pred))\n",
    "    print(\"MSE:\", mean_squared_error(y_true, y_pred))\n",
    "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### 1. SVR\n",
    "Support Vector Regression (RBF Kernel)\n",
    "SVR with an RBF kernel is powerful but computationally expensive for large feature spaces like one-hot encoded protein sequences.\n",
    "We try two settings of the regularization parameter C:\n",
    "\n",
    "For C=1, epsilon=0.1 (the default setting for the SVR in sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:48:27.129414Z",
     "start_time": "2025-11-27T20:48:16.660877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVR (RBF kernel) ---\n",
      "R2: 0.4304178045236362\n",
      "MSE: 0.5782179751867897\n",
      "MAE: 0.6125478750517874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel=\"rbf\", C=1, epsilon=0.1)\n",
    "\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "evaluate_model(\"SVR (RBF kernel)\", y_test, y_pred_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is worse than linear model. But if we reduce regularization parameter C to C=0.1, we will get a better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:48:16.659861Z",
     "start_time": "2025-11-27T20:48:06.527864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVR (RBF kernel) ---\n",
      "R2: 0.6082557674117688\n",
      "MSE: 0.39768370352382193\n",
      "MAE: 0.49260235995353174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel=\"rbf\", C=10, epsilon=0.1)\n",
    "\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "evaluate_model(\"SVR (RBF kernel)\", y_test, y_pred_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multi-Layer Perceptron (Neural Network)\n",
    "A two‑layer neural network is trained with ReLU nonlinearities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:56:44.100209Z",
     "start_time": "2025-11-27T20:53:40.720210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MLP Neural Network ---\n",
      "R2: 0.7996177778725667\n",
      "MSE: 0.20342033803400744\n",
      "MAE: 0.3395473563043571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "evaluate_model(\"MLP Neural Network\", y_test, y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forest Regression\n",
    "Random Forests model nonlinear interactions through an ensemble of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:53:27.811736Z",
     "start_time": "2025-11-27T20:53:23.465426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest ---\n",
      "R2: 0.7234071114426441\n",
      "MSE: 0.28078648041121296\n",
      "MAE: 0.3920988405903088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    n_estimators=20,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "evaluate_model(\"Random Forest\", y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that nonlinear models performs better on the sequence-to-fitness problem compared to linear models. However, parameter selection can affect the model performance a lot. Cross-validation can be used to select better parameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioML-tut-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
